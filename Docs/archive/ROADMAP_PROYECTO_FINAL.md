# üöÄ Roadmap Priorizado - Proyecto Final (ADAPTADO A EVENT MANAGER)

**Objetivo**: Adaptar t√©cnicas del proyecto final del curso a nuestro Event Manager  
**Proyecto Real**: Personal Coordination Voice Agent (Event Manager)  
**T√©cnicas del Curso**: MCP, Web Scraping, Human-in-the-Loop, Multi-Agente  
**Base de datos**: Supabase (ya implementado)  
**Tracing/Evaluaci√≥n**: LangSmith + Langfuse  
**Frontend**: Web simple para probar y visualizar

---

## üéØ ADAPTACI√ìN: De "Curaci√≥n de Contenidos" a "Event Discovery"

**El proyecto del curso es un MOLDE** - debemos adaptarlo a nuestro caso de uso:

| Proyecto Curso (Molde) | Event Manager (Nuestro) |
|------------------------|--------------------------|
| Monitorizar Telegram/WhatsApp | Monitorizar Gmail + Web scraping sitios eventos |
| Extraer URLs gen√©ricas | Extraer URLs de eventos + fechas/lugares |
| Procesar contenido web | Procesar p√°ginas de eventos |
| Publicar posts en web | Sugerir eventos ‚Üí Aprobar ‚Üí Google Calendar |
| Tabla `posts` | Tabla `events_suggested` (o usar `extracted_events`) |

---

## üìã Prioridades (Orden de Implementaci√≥n)

### **Estado Actual del Proyecto** üìä

| Tarea | Estado | Archivos Creados/Modificados |
|-------|--------|------------------------------|
| 1.1 Tabla posts en Supabase | ‚úÖ COMPLETADO | `sql/add_posts_tables.sql` (ejecutado) |
| 1.2 URL Extraction Tool | ‚úÖ COMPLETADO | `app/agents/tools/url_extraction_tool.py`, `app/agents/tools/registry.py`, `app/schemas/tool_schemas.py` |
| 1.3 Web Scraping Tool | ‚úÖ COMPLETADO | `app/agents/tools/web_scraping_tool.py`, `app/agents/tools/registry.py`, `app/schemas/tool_schemas.py` |
| 1.4 EventAgent (adaptado) | ‚úÖ COMPLETADO | `app/agents/specialists/event_agent.py`, `app/agents/specialists/__init__.py` |
| 1.5 API Endpoints (events.py) | ‚úÖ COMPLETADO | `app/api/events.py`, `app/config/database.py` (m√©todos a√±adidos), `app/main.py` |
| 1.6 Frontend (events.html) | ‚úÖ COMPLETADO | `static/events.html` |

---

### **FASE 1: MVP Funcional R√°pido** (3-4 d√≠as) ‚ö°

**Objetivo**: Tener algo funcionando end-to-end lo antes posible

#### 1.1 Tabla de Posts en Supabase (30 min) ‚úÖ
```sql
-- Ejecutar en Supabase SQL Editor
-- Archivo: sql/add_posts_tables.sql (NUEVO - compatible con esquema existente)
-- A√±ade sin modificar tablas existentes:
-- - Tabla posts (con status: pending/approved/rejected/published)
-- - Tabla curation_state (para persistencia de estado)
-- - Vistas: posts_pending, posts_published
-- - RLS policies configuradas
-- - Idempotente (puede ejecutarse m√∫ltiples veces)
```

#### 1.2 URL Extraction Tool (1 hora) ‚úÖ COMPLETADO
```python
# app/agents/tools/url_extraction_tool.py ‚úÖ CREADO
# - Extrae URLs usando regex
# - Valida URLs (http/https)
# - Normaliza URLs (elimina tracking params)
# - Elimina duplicados
# - Registrado en tool_registry
# - A√±adido a TOOL_DEFINITIONS
# Basado en: https://github.com/juananpe/google-image-search-mcp-python
```

#### 1.3 Web Scraping B√°sico (2 horas) ‚úÖ COMPLETADO
```python
# app/agents/tools/web_scraping_tool.py ‚úÖ CREADO
# - Usa httpx + BeautifulSoup
# - Extrae t√≠tulo (Open Graph, Twitter Card, <title>, h1)
# - Extrae descripci√≥n (Open Graph, Twitter Card, meta description)
# - Extrae imagen destacada (Open Graph, Twitter Card, primera imagen grande)
# - Opcional: extrae texto del contenido
# - Headers de navegador real para evitar bloqueos
# - Timeout de 30s y manejo de errores
# - Registrado en tool_registry y TOOL_DEFINITIONS
# Referencia: https://youtu.be/J_T99KC1roI (Playwright MCP para futuro)
```

#### 1.4 EventAgent B√°sico (2 horas) üÜï ADAPTADO
```python
# app/agents/specialists/event_agent.py
# ADAPTADO: En lugar de ContentAgent gen√©rico, EventAgent especializado
# - Usa web_scraping_tool para obtener contenido de URLs de eventos
# - Extrae: t√≠tulo, fecha, hora, lugar, descripci√≥n
# - Genera resumen del evento con LLM (2-3 l√≠neas)
# - Determina relevancia usando RAG + preferencias del usuario
# - Retorna estructura de evento sugerido
# - Puede usar extracted_events existente o crear events_suggested
```

#### 1.5 API Endpoints para Eventos (1 hora) üÜï ADAPTADO
```python
# app/api/events.py (en lugar de posts.py)
# POST /api/v1/events/suggest - Sugerir evento (crea en extracted_events o events_suggested)
# GET /api/v1/events/suggested - Listar eventos sugeridos (status: suggested)
# POST /api/v1/events/{id}/approve - Aprobar ‚Üí Crear en Google Calendar
# POST /api/v1/events/{id}/reject - Rechazar (status: rejected)
# GET /api/v1/events - Listar todos los eventos (sugeridos, aprobados, rechazados)
```

#### 1.6 Frontend B√°sico para Eventos (2 horas) üÜï ADAPTADO
```html
# static/events.html
# - Listar eventos sugeridos (con relevancia score)
# - Vista de aprobaci√≥n con detalles: t√≠tulo, fecha, lugar, descripci√≥n
# - Botones: Aprobar (‚Üí Google Calendar) | Rechazar | Modificar
# - Mostrar imagen del evento si existe
# - Mostrar source_url del evento
```

**Resultado**: Sistema funcional b√°sico para probar el flujo completo

---

### **üìù Notas T√©cnicas de Implementaci√≥n**

#### **URL Extraction Tool - Detalles T√©cnicos** ‚úÖ

**Archivos modificados**:
- ‚úÖ `app/agents/tools/url_extraction_tool.py` - Tool implementado
- ‚úÖ `app/agents/tools/registry.py` - Registrado en `_register_default_tools()`
- ‚úÖ `app/agents/tools/__init__.py` - Exportado en `__all__`
- ‚úÖ `app/schemas/tool_schemas.py` - A√±adido a `TOOL_DEFINITIONS` y `ToolName` enum

**Caracter√≠sticas implementadas**:
- ‚úÖ Extracci√≥n de URLs con regex (http/https)
- ‚úÖ Validaci√≥n de URLs (scheme + netloc)
- ‚úÖ Normalizaci√≥n de URLs (elimina par√°metros de tracking como utm_*)
- ‚úÖ Eliminaci√≥n de duplicados
- ‚úÖ Respuestas estandarizadas (success/error)
- ‚úÖ Logging completo

**Uso**:
```python
from app.agents.tools import tool_registry

result = await tool_registry.execute_tool(
    "extract_urls",
    text="Mira este art√≠culo: https://example.com/article?utm_source=twitter",
    normalize=True,
    remove_duplicates=True
)
# Result: {"success": True, "urls": ["https://example.com/article"], "count": 1}
```

**‚úÖ MVP Event Discovery COMPLETADO** - Todas las tareas principales est√°n implementadas

---

### **FASE 2: Integraci√≥n MCP y Mensajer√≠a** (2-3 d√≠as) üì±

#### 2.1 Event Extraction de Emails (2 horas) üÜï ADAPTADO
```python
# Usar IMAP existente (app/agents/tools/imap_search_tool.py)
# Filtrar emails con informaci√≥n de eventos
# Tool: extract_events_from_emails(limit, since)
# - Buscar emails con palabras clave: "evento", "conferencia", "meeting", etc.
# - Extraer URLs de eventos de emails
# - Extraer fechas/lugares del texto
# - Ya tenemos: imap_search_tool, imap_read_tool
```

#### 2.2 News Scraping Tool (3-4 horas) ‚úÖ COMPLETADO
```python
# app/agents/tools/news_scraping_tool.py ‚úÖ CREADO
# - Scrapea sitios de noticias configurables (TechCrunch, HackerNews, The Verge)
# - Busca menciones de eventos usando keywords (conferencia, event, meetup, etc.)
# - Extrae informaci√≥n de eventos de las noticias
# - Retorna lista de eventos encontrados con informaci√≥n extra√≠da
# - Usa web_scraping_tool internamente
# - Registrado en tool_registry
# Caso de uso: "Conferencia NeurIPS 2025 anunciada" ‚Üí Sugerir evento
```

#### 2.3 Event Site Scraping (2 horas) üÜï ADAPTADO
```python
# Scrapear sitios espec√≠ficos de eventos (configurables)
# - Eventbrite, Meetup, sitios de conferencias
# - Tool: scrape_event_sites(sites, categories)
# - Filtrar eventos relevantes usando RAG
# - Crear eventos sugeridos
# Referencia: https://youtu.be/J_T99KC1roI (Playwright MCP para futuro)
# Por ahora: usar web_scraping_tool existente
```

**Resultado**: Monitorizaci√≥n autom√°tica de mensajes

---

### **FASE 3: LangSmith + Langfuse Integration** (1-2 d√≠as) üìä

#### 3.1 LangSmith Setup (2 horas)
```python
# app/services/tracing.py
# Integrar LangSmith para tracing de LangGraph
# Referencia: https://docs.smith.langchain.com/
# Ejemplo: https://github.com/openai/openai-agents-python
```

**Configuraci√≥n**:
```python
# .env
LANGCHAIN_API_KEY=your_key
LANGCHAIN_PROJECT=personal-coordination-agent
LANGCHAIN_TRACING_V2=true
```

#### 3.2 Langfuse Setup (2 horas)
```python
# app/services/evaluation.py
# Integrar Langfuse para evaluaci√≥n
# Usar con OpenAI Agents SDK
# Referencia: Dia 3/LangFuse/integration_openai_agents.ipynb
```

**Configuraci√≥n**:
```python
# .env
LANGFUSE_PUBLIC_KEY=your_key
LANGFUSE_SECRET_KEY=your_secret
LANGFUSE_HOST=https://cloud.langfuse.com
```

#### 3.3 Evaluadores (2 horas)
```python
# app/evaluators/
# - Evaluador de calidad de res√∫menes (LLM como juez)
# - Evaluador de extracci√≥n de URLs
# - Evaluador de generaci√≥n de im√°genes
```

**Resultado**: Observabilidad completa y evaluaci√≥n autom√°tica

---

### **FASE 4: Human-in-the-Loop Mejorado** (1 d√≠a) üë§

#### 4.1 Interfaz de Aprobaci√≥n Mejorada (3 horas)
```html
# static/approve.html
# - Vista previa de posts con imagen
# - Edici√≥n inline de t√≠tulo/resumen
# - Regenerar imagen
# - Historial de decisiones
```

#### 4.2 Modificaci√≥n de Posts (2 horas)
```python
# POST /api/v1/posts/{id}/modify
# - Regenerar resumen
# - Regenerar imagen
# - Editar t√≠tulo
```

**Resultado**: Interfaz completa para aprobaci√≥n humana

---

### **FASE 5: Scheduling y Automatizaci√≥n** (1 d√≠a) ‚è∞

#### 5.1 Scheduler con APScheduler (2 horas)
```python
# app/scheduler/content_curation_job.py
# Ejecutar diariamente a las 9 AM
# - Leer mensajes nuevos
# - Extraer URLs
# - Procesar y crear posts pendientes
```

#### 5.2 Persistencia de Estado (1 hora)
```python
# Tabla en Supabase: curation_state
# - last_processed_message_id
# - last_execution_time
# - error_log
```

**Resultado**: Ejecuci√≥n autom√°tica diaria

---

## üõ†Ô∏è Estructura de Archivos Propuesta

```
Proyecto/personal-coordination-voice-agent/
‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îú‚îÄ‚îÄ agents/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ specialists/
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ content_agent.py          # NUEVO: Procesamiento de URLs
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ tools/
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ url_extraction_tool.py    # NUEVO: Extraer URLs
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ web_scraping_tool.py      # NUEVO: Scraping b√°sico
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ image_generation_tool.py  # NUEVO: Generar im√°genes
‚îÇ   ‚îú‚îÄ‚îÄ api/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ posts.py                      # NUEVO: Endpoints de posts
‚îÇ   ‚îú‚îÄ‚îÄ mcp/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ servers/
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ telegram_mcp.py          # NUEVO: Telegram MCP
‚îÇ   ‚îú‚îÄ‚îÄ services/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ tracing.py                    # NUEVO: LangSmith
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ evaluation.py                 # NUEVO: Langfuse
‚îÇ   ‚îú‚îÄ‚îÄ scheduler/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ content_curation_job.py      # NUEVO: Job diario
‚îÇ   ‚îî‚îÄ‚îÄ evaluators/
‚îÇ       ‚îú‚îÄ‚îÄ summary_evaluator.py         # NUEVO: Evaluar res√∫menes
‚îÇ       ‚îî‚îÄ‚îÄ url_extraction_evaluator.py  # NUEVO: Evaluar extracci√≥n
‚îú‚îÄ‚îÄ static/
‚îÇ   ‚îú‚îÄ‚îÄ posts.html                        # NUEVO: Visualizar posts
‚îÇ   ‚îî‚îÄ‚îÄ approve.html                      # NUEVO: Aprobar posts
‚îî‚îÄ‚îÄ scripts/
    ‚îî‚îÄ‚îÄ setup_supabase_posts.sql          # NUEVO: Setup DB
```

---

## üìö Recursos del Curso a Usar

### **Repositorios y Ejemplos**:
1. **Google Image Search MCP**: https://github.com/juananpe/google-image-search-mcp-python
   - Ejemplo de servidor MCP en Python
   - Estructura para Telegram MCP

2. **Research Server MCP**: https://gist.github.com/juananpe/a9f13d7d17eb7202e1f3cc3ce4ef400e
   - Ejemplo de servidor MCP completo
   - Patr√≥n a seguir

3. **Cliente MCP en Python**: https://gist.github.com/juananpe/588b0967cd6f1ed3385e56f81ed87896
   - Cliente MCP general
   - Ya tenemos implementado, pero podemos mejorar

4. **OpenAI Agents SDK**: https://github.com/openai/openai-agents-python
   - Ejemplos de multi-agente
   - Integraci√≥n con MCP

### **V√≠deos Tutoriales**:
- **Playwright MCP**: https://youtu.be/J_T99KC1roI
- **Implementando MCP**: https://youtu.be/UEZABGkibh0
- **MCP Avanzado**: https://youtu.be/z08J43j94WQ

### **Documentaci√≥n**:
- **OpenAI Function Calling**: https://platform.openai.com/docs/guides/function-calling
- **Anthropic Tool Use**: https://docs.anthropic.com/en/docs/agents-and-tools/tool-use/overview
- **LangSmith**: https://docs.smith.langchain.com/
- **Langfuse**: https://langfuse.com/docs

---

## üéØ Plan de Implementaci√≥n R√°pida (MVP en 3-4 d√≠as)

### **D√≠a 1: Fundamentos**
- ‚úÖ Crear tabla `posts` en Supabase
- ‚úÖ URL Extraction Tool
- ‚úÖ Web Scraping Tool b√°sico (requests + BeautifulSoup)
- ‚úÖ API endpoints b√°sicos

### **D√≠a 2: Procesamiento**
- ‚úÖ ContentAgent b√°sico
- ‚úÖ Generaci√≥n de res√∫menes
- ‚úÖ Extracci√≥n de im√°genes
- ‚úÖ Frontend b√°sico para visualizar

### **D√≠a 3: Integraci√≥n**
- ‚úÖ Telegram/WhatsApp reading
- ‚úÖ Flujo completo: mensaje ‚Üí URL ‚Üí post ‚Üí aprobaci√≥n
- ‚úÖ Testing end-to-end

### **D√≠a 4: Pulido**
- ‚úÖ LangSmith/Langfuse integration
- ‚úÖ Interfaz de aprobaci√≥n mejorada
- ‚úÖ Documentaci√≥n

---

## üîß Configuraci√≥n Inicial

### **1. Variables de Entorno (.env)**
```bash
# Ya existentes
SUPABASE_URL=...
SUPABASE_ANON_KEY=...
SUPABASE_SERVICE_ROLE_KEY=...

# Nuevas para el proyecto
# Telegram
TELEGRAM_BOT_TOKEN=...
TELEGRAM_CHAT_ID=...

# LangSmith
LANGCHAIN_API_KEY=...
LANGCHAIN_PROJECT=personal-coordination-agent
LANGCHAIN_TRACING_V2=true

# Langfuse
LANGFUSE_PUBLIC_KEY=...
LANGFUSE_SECRET_KEY=...
LANGFUSE_HOST=https://cloud.langfuse.com

# OpenAI (para generaci√≥n de im√°genes)
OPENAI_API_KEY=...
```

### **2. Dependencias (requirements.txt)**
```txt
# A√±adir estas dependencias
beautifulsoup4>=4.12.0
python-telegram-bot>=20.0
langsmith>=0.1.0
langfuse>=2.0.0
playwright>=1.40.0  # Para MCP Playwright
```

---

## üöÄ Quick Start: Implementar MVP

### **Paso 1: Setup Supabase (5 min)**
```sql
-- Ejecutar en Supabase SQL Editor
-- (Ver SQL arriba en Fase 1.1)
```

### **Paso 2: URL Extraction Tool (30 min)**
```python
# Crear app/agents/tools/url_extraction_tool.py
# Implementar extracci√≥n con regex
```

### **Paso 3: Web Scraping Tool (1 hora)**
```python
# Crear app/agents/tools/web_scraping_tool.py
# Usar requests + BeautifulSoup
```

### **Paso 4: ContentAgent (1 hora)**
```python
# Crear app/agents/specialists/content_agent.py
# Integrar con LLM para res√∫menes
```

### **Paso 5: API Endpoints (30 min)**
```python
# Crear app/api/posts.py
# Endpoints b√°sicos CRUD
```

### **Paso 6: Frontend (1 hora)**
```html
# Crear static/posts.html
# Visualizaci√≥n b√°sica
```

**Total**: ~4 horas para MVP funcional

---

## üìä Integraci√≥n LangSmith + Langfuse

### **LangSmith (Tracing)**
```python
# app/services/tracing.py
from langsmith import traceable
from langchain_core.tracers import LangChainTracer

# Configurar en settings.py
langsmith_api_key: str = Field(default="", env="LANGCHAIN_API_KEY")
langsmith_project: str = Field(default="personal-coordination-agent", env="LANGCHAIN_PROJECT")

# Usar en agent_orchestrator
@traceable(name="content_curation")
async def process_url(url: str):
    # Tu c√≥digo aqu√≠
    pass
```

### **Langfuse (Evaluaci√≥n)**
```python
# app/services/evaluation.py
from langfuse import Langfuse
from langfuse.decorators import langfuse_context, observe

# Configurar
langfuse = Langfuse(
    public_key=settings.langfuse_public_key,
    secret_key=settings.langfuse_secret_key,
    host=settings.langfuse_host
)

# Decorar funciones
@observe(name="generate_summary")
async def generate_summary(content: str):
    # Tu c√≥digo aqu√≠
    pass
```

### **Evaluadores**
```python
# app/evaluators/summary_evaluator.py
# Usar LLM como juez para evaluar calidad de res√∫menes
# Comparar resumen generado vs contenido original
```

---

## üé® Frontend Web Simple

### **posts.html** (Visualizaci√≥n)
```html
<!DOCTYPE html>
<html>
<head>
    <title>Posts Publicados</title>
    <style>
        /* CSS moderno tipo cards */
        .post-card {
            border: 1px solid #ddd;
            border-radius: 8px;
            padding: 16px;
            margin: 16px 0;
        }
    </style>
</head>
<body>
    <h1>Posts Publicados</h1>
    <div id="posts-container"></div>
    
    <script>
        // Fetch desde Supabase o API
        async function loadPosts() {
            const response = await fetch('/api/v1/posts?status=published');
            const posts = await response.json();
            // Renderizar cards
        }
        loadPosts();
    </script>
</body>
</html>
```

### **approve.html** (Aprobaci√≥n)
```html
<!DOCTYPE html>
<html>
<head>
    <title>Aprobar Posts</title>
</head>
<body>
    <h1>Posts Pendientes de Aprobaci√≥n</h1>
    <div id="pending-posts"></div>
    
    <script>
        // Cargar posts pendientes
        // Botones aprobar/rechazar/modificar
    </script>
</body>
</html>
```

---

## ‚úÖ Checklist de Implementaci√≥n

### **Fase 1: MVP** (3-4 d√≠as)
- [ ] Tabla `posts` en Supabase
- [ ] URL Extraction Tool
- [ ] Web Scraping Tool
- [ ] ContentAgent b√°sico
- [ ] API endpoints
- [ ] Frontend b√°sico

### **Fase 2: MCP** (2-3 d√≠as)
- [ ] WhatsApp reading
- [ ] Telegram MCP server
- [ ] Playwright MCP integration

### **Fase 3: Observabilidad** (1-2 d√≠as)
- [ ] LangSmith setup
- [ ] Langfuse setup
- [ ] Evaluadores b√°sicos

### **Fase 4: Human-in-the-Loop** (1 d√≠a)
- [ ] Interfaz de aprobaci√≥n mejorada
- [ ] Modificaci√≥n de posts

### **Fase 5: Automatizaci√≥n** (1 d√≠a)
- [ ] Scheduler
- [ ] Persistencia de estado

---

## üéØ Pr√≥ximos Pasos Inmediatos

1. **Crear tabla en Supabase** (5 min)
2. **Implementar URL Extraction Tool** (30 min)
3. **Implementar Web Scraping b√°sico** (1 hora)
4. **Crear ContentAgent** (1 hora)
5. **A√±adir endpoints API** (30 min)
6. **Frontend b√°sico** (1 hora)

**Total**: ~4 horas para tener algo funcional

---

## üìù Notas Importantes

1. **Supabase en lugar de Flask+SQLite**: Ya tenemos Supabase, us√©moslo directamente
2. **LangSmith + Langfuse**: Ambos son √∫tiles, LangSmith para tracing, Langfuse para evaluaci√≥n
3. **Playwright MCP**: Usar cuando sea necesario para contenido din√°mico, pero empezar con requests+BeautifulSoup
4. **Telegram MCP**: Basarse en ejemplos del curso (google-image-search-mcp-python)
5. **Frontend simple**: Empezar con HTML/JS vanilla, luego mejorar si es necesario

---

**¬øEmpezamos con la Fase 1 (MVP)?** üöÄ

